{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of conditional probability and its application on Text\n",
    "\n",
    "- Assume this small dataset is given:\n",
    "\n",
    "<img src=\"../Notebooks/Images/spam_ham_data_set.png\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Create spam and ham dictionary\n",
    "\n",
    "- Create two dictionaries for spam and ham where keys are unique words and values are the frequency of each word\n",
    "    - Example: if the word \"password\" shows up 4 times in the text, then in the dictionary, the key would be \"password\" and the value would be 4\n",
    "- Create the dictionaries programatically using `for` loops\n",
    "- Use the below text to create your dictionaries:\n",
    "    - `spam_text= ['Send us your password', 'review us', 'Send your password', 'Send us your account']`\n",
    "    - `ham_text= ['Send us your review', 'review your password']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Dictionary\n",
      "{'send': 3, 'us': 3, 'your': 3, 'password': 2, 'review': 1, 'account': 1}\n",
      "\n",
      "\n",
      "Ham Dictionary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'send': 1, 'us': 1, 'your': 2, 'review': 2, 'password': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_text = ['Send us your password', 'review us', 'Send your password', 'Send us your account']\n",
    "ham_text = ['Send us your review', 'review your password']\n",
    "\n",
    "spam = {}\n",
    "ham = {}\n",
    "\n",
    "for item in spam_text:\n",
    "    for j in item.lower().split(' '):\n",
    "        if j not in spam:\n",
    "            spam[j] = 1\n",
    "        else:\n",
    "            spam[j] += 1\n",
    "print('Spam Dictionary')\n",
    "print(spam)\n",
    "print(\"\\n\")\n",
    "for item in ham_text:\n",
    "    for j in item.lower().split(' '):\n",
    "        if j not in ham:\n",
    "            ham[j] = 1\n",
    "        else:\n",
    "            ham[j] += 1\n",
    "print('Ham Dictionary')            \n",
    "ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Given our dictionaries from the last activity, if we know an email is spam, what is the probability that the word \"password\" is in the email? \n",
    "\n",
    "What is the frequency of \"password\" in a spam email?\n",
    "\n",
    "- Answer:\n",
    "\n",
    " $P(password \\mid spam) = 2/(3+3+3+2+1+1) = 2/13 \\approx 15.38\\%$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15384615384615385\n"
     ]
    }
   ],
   "source": [
    "p_password_given_spam = spam['password']/sum(spam.values())\n",
    "print(p_password_given_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Given our dictionaries from the last activity, if we know an email is ham, what is the probability that the word \"password\" is in the email? \n",
    "\n",
    "What is the frequency of \"password\" in a ham email?\n",
    "\n",
    "- Answer:\n",
    "\n",
    "$P(password \\mid ham) = 1/(1+2+1+1+2+0) = 1/7 \\approx 14.29\\%$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Assume we have seen the word \"password\" in an email, what is the probability that the email is spam?\n",
    "\n",
    "- $P(spam \\mid password) = ?$\n",
    "- Hint: Use Bayes' rule and Law of Total Probability (LOTP):\n",
    "    - Bayes' Rule: $P(spam \\mid password) = (P(password \\mid spam) P(spam))/ P(password)$ \n",
    "    - LOTP: $P(password) = P(password \\mid spam) P(spam) + P(password \\mid ham) P(ham)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Apply the naive Bayes to spam/ham email dataset:\n",
    "\n",
    "**In groups of 3, complete the following activity**\n",
    "\n",
    "1. Please read this article, starting at the **Naive Bayes Assumption** section: https://pythonmachinelearning.pro/text-classification-tutorial-with-naive-bayes/\n",
    "1. We will use the [Spam Dataset](Datasets/spam.csv)\n",
    "1. In the article, for the codeblock of the `fit` method, which line(s) of the method calculates the probabilty of ham and spam?\n",
    "1. For the same `fit` method, which line(s) of the method calculates the spam and ham dictionaries?\n",
    "1. In the article, for the codeblock of the `predict` method, which line(s) compares the scores of ham or spam based on log probabilities?\n",
    "\n",
    "We will discuss as a class after workinging in groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Find the Naive Bayes core parts in the SpamDetector Class\n",
    "\n",
    "**In groups of 3, complete the following activity**\n",
    "\n",
    "Assume we have written the `SpamDetector` class from the article. Train this model from the given [Spam Dataset](Datasets/spam.csv), and use it to make a prediction!\n",
    "\n",
    "Use the starter code below, and then fill in the TODOs in the `main`.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- you will need to use [train_test_split from sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to obtain your training and test (prediction) data\n",
    "- You will need to instantiate your `SpamDetector`, fit the training data to it, predict using the test values, and then measure the accuracy\n",
    "- To calculate accuracy: add up all the correct predictions divided by the total number of predictions\n",
    "- Use the following code to get your data ready for transforming/manipulating:\n",
    "```\n",
    "data = pd.read_csv('Datasets/spam.csv',encoding='latin-1')\n",
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "print(data.head())\n",
    "tags = data[\"label\"]\n",
    "texts = data[\"text\"]\n",
    "X, y = texts, tags\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "data = pd.read_csv('Datasets/spam.csv',encoding='latin-1')\n",
    "data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1, inplace=True)\n",
    "data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = data[\"label\"]\n",
    "texts = data[\"text\"]\n",
    "X, y = texts, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9704035874439462\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "class SpamDetector(object):\n",
    "    \"\"\"Implementation of Naive Bayes for binary classification\"\"\"\n",
    "\n",
    "    # clean up our string by removing punctuation\n",
    "    def clean(self, s):\n",
    "        translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        return s.translate(translator)\n",
    "\n",
    "    #  tokenize our string into words\n",
    "    def tokenize(self, text):\n",
    "        text = self.clean(text).lower()\n",
    "        return re.split(\"\\W+\", text)\n",
    "\n",
    "    # count up how many of each word appears in a list of words.\n",
    "    def get_word_counts(self, words):\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0.0) + 1.0\n",
    "        return word_counts\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        # obtaining probablities and making ham and spam dictionaries\n",
    "        \"\"\"Fit our classifier\n",
    "        Arguments:\n",
    "            X {list} -- list of document contents\n",
    "            y {list} -- correct labels\n",
    "        \"\"\"\n",
    "        self.num_messages = {}\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        # Compute log class priors (the probability that any given message is spam/ham),\n",
    "        # by counting how many messages are spam/ham, \n",
    "        # dividing by the total number of messages, and taking the log.\n",
    "        n = len(X)\n",
    "        self.num_messages['spam'] = sum(1 for label in Y if label == 'spam')\n",
    "        self.num_messages['ham'] = sum(1 for label in Y if label == 'ham')\n",
    "        self.log_class_priors['spam'] = math.log(self.num_messages['spam'] / n )\n",
    "        self.log_class_priors['ham'] = math.log(self.num_messages['ham'] / n )\n",
    "        self.word_counts['spam'] = {}\n",
    "        self.word_counts['ham'] = {}\n",
    "\n",
    "        # for each (document, label) pair, tokenize the document into words.\n",
    "        for x, y in zip(X, Y):\n",
    "            c = 'spam' if y == 'spam' else 'ham'\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            # For each word, either add it to the vocabulary for spam/ham, \n",
    "            # if it isnâ€™t already there, and update the number of counts. \n",
    "            for word, count in counts.items():\n",
    "                # Add that word to the global vocabulary.\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.add(word)\n",
    "                if word not in self.word_counts[c]:\n",
    "                    self.word_counts[c][word] = 0.0\n",
    "\n",
    "                self.word_counts[c][word] += count\n",
    "\n",
    "    # function to actually output the class label for new data.\n",
    "    def predict(self, X):\n",
    "        # adding log of the word given the email is spam \n",
    "        result = []\n",
    "        # Given a document...\n",
    "        for x in X:\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            spam_score = 0\n",
    "            ham_score = 0\n",
    "            # We iterate through each of the words...\n",
    "            for word, _ in counts.items():\n",
    "                if word not in self.vocab: continue\n",
    "                # ... and compute log p(w_i|Spam), and sum them all up. The same will happen for Ham\n",
    "                # add Laplace smoothing\n",
    "                # https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf\n",
    "                log_w_given_spam = math.log( (self.word_counts['spam'].get(word, 0.0) + 1) / (self.num_messages['spam'] + len(self.vocab)) )\n",
    "                log_w_given_ham = math.log( (self.word_counts['ham'].get(word, 0.0) + 1) / (self.num_messages['ham'] + len(self.vocab)) )\n",
    "\n",
    "                spam_score += log_w_given_spam\n",
    "                ham_score += log_w_given_ham\n",
    "            \n",
    "            # Then we add the log class priors...\n",
    "            spam_score += self.log_class_priors['spam']\n",
    "            ham_score += self.log_class_priors['ham']\n",
    "\n",
    "            # ... and check to see which score is bigger for that document.\n",
    "            # Whichever is larger, that is the predicted label!\n",
    "            if spam_score > ham_score:\n",
    "                result.append('spam')\n",
    "            else:\n",
    "                result.append('ham')\n",
    "        return result\n",
    "        \n",
    "\n",
    "# TODO: Fill in the below function to make a prediction, \n",
    "# your answer should match the final number in the below output (0.9641)\n",
    "if __name__ == '__main__':\n",
    "    from sklearn.model_selection import train_test_split \n",
    "    import numpy as np \n",
    "    sd = SpamDetector()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=1)\n",
    "\n",
    "    sd.fit(X_train, y_train)\n",
    "    print(np.mean(np.array(sd.predict(X_test)) == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: use sklearn CountVectorizer and MultinomialNB to spam email dataset\n",
    "\n",
    "As we've seen with previous topics, sklearn has a lot of built in functionality that can save us from writing the code from scratch. We are going to solve the same problem in the previous activity, but using sklearn!\n",
    "\n",
    "For example, the `SpamDectector` class in the previous activity is an example of a **Multinomial Naive Bayes (MNB) model**. An MNB lets us know that each conditional probability we're looking at (i.e. $P(spam | w_1, w_2, ..., w_n)$) is a multinomial (several terms, polynomial) distribution, rather than another type distribution.\n",
    "\n",
    "**In groups of 3, complete the activity by using the provided starter code and following the steps below:**\n",
    "\n",
    "1 - Split the dataset\n",
    "\n",
    "`from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)`\n",
    "\n",
    "2 - Vectorize the dataset : `vect = CountVectorizer()`\n",
    "\n",
    "3 - Transform training data into a document-term matrix (BoW): `X_train_dtm = vect.fit_transform(X_train)`\n",
    "\n",
    "4 - Build and evaluate the model\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- Remember how you prepared/cleaned/labeled the dataset, created texts and tags, and split the data innto train vs test from the previous activity. You'll need to do so again here\n",
    "- Review the [CountVectorizer documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to see how you can transform text into numerical vectors\n",
    "- Need more help? Check out this [MNB Vectorization](https://www.ritchieng.com/machine-learning-multinomial-naive-bayes-vectorization/) article and see what you can use from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9874439461883409"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## starter code:\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# TODO: Prepare the dataset\n",
    "data = pd.read_csv('Datasets/spam.csv',encoding='latin-1')\n",
    "data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1, inplace=True)\n",
    "data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "# TODO: create texts and tags\n",
    "tags = data[\"label\"]\n",
    "texts = data[\"text\"]\n",
    "X, y = texts, tags\n",
    "# TODO: split the data into train vs test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# TODO: transform text into numerical vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_dtm = vectorizer.fit_transform(X_train)\n",
    "# print(X_train_dtm[:1])\n",
    "# print(X_train[:1])\n",
    "# instantiate Multinomial Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "# fit to model, with the trained part of the dataset\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "X_test_dtm = vectorizer.transform(X_test)\n",
    "# make prediction\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "# test accurarcy of prediction\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[947,   2],\n",
       "       [ 12, 154]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_class)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n        Spam       0.99      1.00      0.99       949\\n         Ham       0.99      0.93      0.96       166\\n\\n    accuracy                           0.99      1115\\n   macro avg       0.99      0.96      0.97      1115\\nweighted avg       0.99      0.99      0.99      1115\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_name = ['Spam', 'Ham']\n",
    "classification_report(y_test, y_pred_class, target_names=target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
