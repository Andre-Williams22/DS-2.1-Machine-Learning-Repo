{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: What are the pre-processings to apply a machine learning algorithm on text data?\n",
    "\n",
    "1. The text must be parsed to words, called tokenization\n",
    "\n",
    "2. Then the words need to be encoded as integers or floating point values\n",
    "\n",
    "3. scikit-learn library offers easy-to-use tools to perform both tokenization and feature extraction of text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is TF-IDF Vectorizer?\n",
    "\n",
    "- Word counts are a good starting point, but are very basic\n",
    "\n",
    "An alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF. \n",
    "\n",
    "**Term Frequency**: This summarizes how often a given word appears within a document\n",
    "\n",
    "**Inverse Document Frequency**: This downscales words that appear a lot across documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuitive idea behind TF-IDF:\n",
    "    \n",
    "- If a word appears frequently in a document, it's important. Give the word a high score\n",
    "\n",
    "- But if a word appears in many documents, it's not a unique identifier. Give the word a low score\n",
    "\n",
    "<img src=\"../Notebooks/Images/tfidf_slide.png\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Obtain the keywords from TF-IDF\n",
    "\n",
    "1- First obtain the TF-IDF matrix for given corpus\n",
    "\n",
    "2- Do column-wise addition\n",
    "\n",
    "3- Sort the score from highest to lowest\n",
    "\n",
    "4- Return the associated words based on step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.78528828 0.         0.         0.6191303  0.        ]\n",
      " [0.         0.70710678 0.         0.         0.70710678]\n",
      " [0.         0.53256952 0.         0.65782931 0.53256952]\n",
      " [0.         0.36626037 0.57381765 0.         0.73252075]]\n",
      "['blue', 'bright', 'shining', 'sky', 'sun']\n",
      "[('shining', 1.7229683601509618), ('sky', 1.672598769332993), ('bright', 1.414213562373095)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "documents = ['The sky is blue', 'The sun is bright', 'The sun in the sky is bright', 'we can see the shining sun, the bright sun']\n",
    "\n",
    "# finds top keywords in our list\n",
    "# there are no word orders with tfidvectorizers and count vectorizers \n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tf_idf_matrix = vectorizer.fit_transform(documents)\n",
    "print(tf_idf_matrix.toarray());\n",
    "print(vectorizer.get_feature_names()) ## grabs names of features\n",
    "# first column is bright\n",
    "# the score goes down as the weight of score decrease \n",
    "column_score = np.sum(tf_idf_matrix, axis=1)\n",
    "sorted_words = np.sort(column_score)\n",
    "tfidf_scores = np.ravel(sorted_words)\n",
    "# returns top 3 words \n",
    "print(sorted(dict(zip(vectorizer.get_feature_names(), tfidf_scores)).items(), key=lambda x: x[1], reverse=True)[:3])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn,, Gensim and keras have a built in tfidf score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "- Data Scientists have assigned a vector to each english word\n",
    "\n",
    "- This process of assignning vectors to each word is called Word2Vec\n",
    "\n",
    "- In DS 2.4, we will learn how they accomplished Word2Vec task\n",
    "\n",
    "- Download this huge Word2Vec file: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "- Do not open the extracted file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the property of vectors associated to each word in Word2Vec?\n",
    "\n",
    "- Words with similar meanings would be closer to each other in Euclidean Space\n",
    "\n",
    "- For example if $V_{pizza}$, $V_{food}$ and $V_{sport}$ represent the vector associated to pizza, food and sport then:\n",
    "\n",
    "${\\| V_{pizza} - V_{food}}\\|$ < ${\\| V_{pizza} - V_{sport}}\\|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 1), ('h', 2), ('a', 5), ('c', 15)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtable = {'h':2, 'a':5, 'b':1, 'c':15}\n",
    "\n",
    "           \n",
    "a = sorted(hashtable.items(), key=lambda x: x[1]) \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-86128bcb2adb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhashtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m##hashtable = sorted(hashtable.items()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "for key,value in hashtable.items():\n",
    "    ##hashtable = sorted(hashtable.items()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
